{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load keras modules and packages\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read MNIST dataset and reshaping\n",
    "\n",
    "# Train, Test 데이터 Load \n",
    "(X_train, Y_train), (X_validation, Y_validation) = mnist.load_data()\n",
    "\n",
    "# Train 데이터 포맷 변환 \n",
    "# 60000(Train Sample 수) * 28(가로) * 28(세로) 포맷\n",
    "# X_train.shape[0] - Train 샘플 수 \n",
    "# Feature Scaling \n",
    "# X_train의 각 원소는 0-255 사이의 값을 가지고 있다 \n",
    "# Overfitting 방지 및 Cost 함수의 빠른 수렴을 위해서 \n",
    "# Feature Scaling 작업을 한다. \n",
    "# 예제에서는 0-255 범위를 0-1 범위로 Scaling\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0],\n",
    "                          28, 28, 1).astype('float32') / 255\n",
    "X_validation = X_validation.reshape(X_validation.shape[0],\n",
    "                                    28, 28, 1).astype('float32') / 255\n",
    "\n",
    "# Lable의 categorical 값을 One-hot 형태로 변환 \n",
    "# 예를 들어 [1, 3, 2, 0] 를 \n",
    "# [[ 0., 1., 0., 0.], \n",
    "# [ 0., 0., 0., 1.], \n",
    "# [ 0., 0., 1., 0.], \n",
    "# [ 1., 0., 0., 0.]] \n",
    "# 로 변환하는 것을 One-hot 형태라고 함 \n",
    "# MNIST Label인 0 ~ 9사이의 10가지 값을 변환한다\n",
    "\n",
    "Y_train = np_utils.to_categorical(Y_train, 10)\n",
    "Y_validation = np_utils.to_categorical(Y_validation, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\PSM\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\PSM\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PSM\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, kernel_size=(3, 3), input_shape=(28, 28, 1..., activation=\"relu\", kernel_initializer=\"glorot_uniform\")`\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\PSM\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", kernel_initializer=\"glorot_uniform\")`\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\PSM\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(64, activation=\"relu\", kernel_initializer=\"glorot_uniform\")`\n",
      "C:\\Users\\PSM\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(64, activation=\"relu\", kernel_initializer=\"glorot_uniform\")`\n"
     ]
    }
   ],
   "source": [
    "# Construct model\n",
    "# Multilayer Perceptron (MLP) 생성\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# glorot_uniform == Xavier Initialization\n",
    "\n",
    "# 첫 번째 Layer \n",
    "model.add(Conv2D(64, kernel_size=(3, 3), input_shape=(28, 28, 1), init='glorot_uniform', activation='relu'))\n",
    "\n",
    "# 두 번째 Layer (Hidden layer 1)\n",
    "model.add(Conv2D(64, (3, 3),  init='glorot_uniform', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "\n",
    "# 세 번째 Layer (Hidden layer 2)\n",
    "model.add(Dense(64,  init='glorot_uniform', activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# 네 번째 Layer (Hidden layer 3)\n",
    "model.add(Dense(64,  init='glorot_uniform', activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Cost function 및 Optimizer 설정 \n",
    "# Multiclass 분류이므로 Cross-entropy 사용 \n",
    "# SGD optimizer 사용\n",
    "# optimezer : SGD ( learning rate = 0.01 )\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01) \n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\PSM\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# Model training \n",
    "# batch size = 200 \n",
    "# epochs = 50 \n",
    "\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_validation, Y_validation), \n",
    "                    epochs=50, batch_size=200, verbose=0)\n",
    "\n",
    "print('\\nAccuracy:{:.4f}'.format(model.evaluate(X_validation, Y_validation)[1]))\n",
    "y_vloss = history.history['val_loss']\n",
    "y_loss = history.history['loss']\n",
    "\n",
    "# model evaluation\n",
    "# Learning curve\n",
    "\n",
    "x_len = np.arange(len(y_loss))\n",
    "plt.plot(x_len, y_loss, marker='.', c='blue', label=\"Train-set Loss\")\n",
    "plt.plot(x_len, y_vloss, marker='.', c='red',\n",
    "         label=\"Validation-set Loss\")\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
